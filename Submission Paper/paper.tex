\documentclass{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usepackage{enumitem}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{hyperref}

\title{A Unique Pipeline for Low Resolution Facial Recognition}
\author{Daniel Szurek, Brandon Nguyen}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Low-resolution facial recognition (LRFR) remains a critical challenge in real-world surveillance and security systems where imaging conditions are suboptimal. This paper presents an end-to-end pipeline that integrates identity-aware super-resolution with efficient facial recognition to address the very-low-resolution (VLR) face recognition problem. Our approach combines a Hybrid Transformer-CNN Deep Super-Resolution (DSR) network, utilizing Max-Feature-Map (MFM) activation for efficient feature selection and transformer attention for global context, with EdgeFace, a lightweight recognition backbone optimized for embedded deployment. Unlike conventional approaches that treat super-resolution and recognition as independent modules, we introduce a three-phase cycle training strategy where DSR and EdgeFace are co-optimized: DSR learns to preserve features that EdgeFace recognizes best, and EdgeFace adapts to the super-resolved distribution. We incorporate multi-scale perceptual loss, identity cosine embedding loss, cross-photo identity consistency, and discriminative supervision to bridge the domain gap. Evaluated on {INSERT DATASET NAME HERE} with probe images at 16×16, 24×24, and 32×32 resolutions, our pipeline demonstrates progressive improvement in recognition accuracy. Our 32×32 configuration achieves {INSERT METRICS HERE}, while maintaining real-time inference capability suitable for edge devices. Our contributions include: (1) a lightweight Hybrid DSR architecture (< 5.5M parameters) with identity-preserving supervision, (2) a cyclic training methodology coupling super-resolution and recognition optimization, and (3) comprehensive evaluation across three VLR resolutions.
\end{abstract}

\begin{IEEEkeywords}
low-resolution face recognition, deep learning
\end{IEEEkeywords}

\section{Introduction}

\renewcommand\thesubsection{\thesection.\Alph{subsection}}

\subsection{Background and Motivation}

Facial recognition systems have become ubiquitous in security, surveillance, and access control applications. However, real-world deployment scenarios frequently encounter significant challenges due to imaging constraints: cameras positioned at large distances capture subjects at low resolution, poor lighting conditions degrade image quality, and cost-effective surveillance infrastructure employs sensors with limited pixel density. These factors collectively create the very-low-resolution facial recognition (VLR-FR) problem, where probe images may be as small as 16×16 or 32×32 pixels—far below the 112×112 input size expected by modern deep face recognition models.

Traditional face recognition pipelines, trained predominantly on high-resolution datasets like MS-Celeb-1M and VGGFace2, exhibit severe performance degradation when presented with VLR inputs. The core issue is twofold: first, critical discriminative features such as fine-grained facial textures, subtle expression patterns, and localized geometric cues are irreversibly lost during severe downsampling; second, the distribution shift between high-resolution training data and low-resolution test data creates a domain gap that deep networks struggle to bridge. Simply upsampling VLR images with bicubic interpolation before recognition yields marginal improvements, as it introduces smoothing artifacts without recovering lost high-frequency details.

Super-resolution (SR) techniques offer a promising pathway to address VLR-FR by reconstructing plausible high-resolution details from low-resolution inputs. However, generic SR models optimized for perceptual quality metrics (PSNR, SSIM) do not necessarily preserve identity-discriminative features required for recognition. A super-resolved face may appear visually pleasing yet fail to maintain the embedding consistency needed for accurate matching against a gallery. This disconnect motivates the need for \textit{identity-aware super-resolution}—SR models explicitly trained to preserve features that recognition networks rely upon.

Our work is motivated by three key observations: (1) SR and recognition stages should be jointly optimized rather than treated as independent modules, (2) lightweight architectures are essential for deployment on edge devices with limited compute budgets, and (3) domain adaptation between SR outputs and recognition model expectations significantly impacts end-to-end accuracy. We address these challenges through a novel pipeline coupling a Hybrid Transformer-CNN Deep Super-Resolution (DSR) network with EdgeFace, a compact recognition backbone, unified via identity-preserving training objectives and cycle optimization.

\subsection{Novelty and Contributions}

This paper makes the following contributions to the VLR facial recognition problem:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Hybrid Transformer-CNN DSR Architecture:} We introduce a lightweight DSR architecture (< 5.5M parameters) that combines the local feature extraction efficiency of CNNs with the global context modeling of Transformers. We employ Max-Feature-Map (MFM) activation for feature selection and multi-head attention to hallucinate plausible facial structures.
    
    \item \textbf{Cycle Training Strategy:} We propose a three-phase training methodology where EdgeFace is fine-tuned on DSR outputs, and DSR is retrained using the fine-tuned EdgeFace as a supervisor. This creates a feedback loop enabling co-adaptation between SR and recognition stages.
    
    \item \textbf{Identity-Aware Loss Functions:} We optimize DSR using a comprehensive set of losses including multi-scale perceptual loss, identity cosine embedding loss, cross-photo identity consistency, and discriminative supervision to ensure embedding consistency.
    
    \item \textbf{Multi-Resolution Scalability:} We train separate DSR models for three VLR input resolutions (16×16, 24×24, 32×32), demonstrating the pipeline's effectiveness across various degradation levels.
    
    \item \textbf{Comprehensive Evaluation:} We provide detailed verification and identification metrics including ROC-AUC, EER, and Rank-1 accuracy, alongside computational efficiency analysis suitable for edge deployment.
\end{enumerate}

\section{Related Work}

\subsection{Deep Learning for Super-Resolution}

Single image super-resolution (SISR) has progressed rapidly with the advent of deep learning. Early work by Dong et al.~\cite{srcnn} introduced SRCNN, a three-layer CNN that learned end-to-end mapping from low to high resolution. Subsequent architectures like VDSR~\cite{vdsr} and EDSR~\cite{edsr} increased network depth and removed batch normalization to improve gradient flow. Kim et al.'s DRCN~\cite{drcn} employed recursive layers for parameter efficiency.

Perceptual loss, introduced by Johnson et al.~\cite{perceptual_loss}, shifted focus from pixel-wise MSE to feature-level similarity using pretrained VGG networks. SRGAN~\cite{srgan} combined perceptual loss with adversarial training to generate photo-realistic textures. More recent work like ESRGAN~\cite{esrgan} and RealSR~\cite{realsr} refine GAN training for better perceptual quality.

For face super-resolution specifically, FSRNet~\cite{fsrnet} and SPARNet~\cite{sparnet} incorporate facial priors (landmarks, parsing maps) to guide reconstruction. However, these methods often optimize for visual quality rather than recognition performance.

\subsection{Low-Resolution Face Recognition}

Low-resolution face recognition has been approached from multiple angles. Early methods like Hennings-Yeomans et al.~\cite{hennings2007} employed simultaneous SR and recognition. Biswas et al.~\cite{biswas2011} proposed multidimensional scaling to learn a common embedding space.

With deep learning, Li et al.'s Coupled-GAN~\cite{coupledgan} jointly trained SR and recognition networks. Chen et al.'s LCNN~\cite{lcnn} introduced attention mechanisms. More recently, He et al.'s DSR framework~\cite{dsr} demonstrated that residual dense blocks with identity loss improve recognition accuracy.

Domain adaptation approaches like DRAN~\cite{dran} and DADE~\cite{dade} learn resolution-invariant features. Our approach simplifies the pipeline by explicitly training SR to preserve recognition features through identity-aware losses and cycle training.

\subsection{Efficient Face Recognition Models}

The recognition stage of our pipeline leverages lightweight architectures suitable for edge deployment. MobileFaceNets~\cite{mobilefacenet} introduced depthwise separable convolutions. ShuffleFaceNet~\cite{shufflefacenet} employed channel shuffle operations.

EdgeFace~\cite{edgeface}, our chosen backbone, balances efficiency and accuracy through carefully designed bottleneck blocks and lightweight depthwise convolutions (LDC). We use the EdgeFace-XXS variant with 1.24M parameters and 512-dimensional embeddings.

\section{Methodology}

Our pipeline consists of three key components: (1) the Hybrid Transformer-CNN DSR network for upsampling VLR inputs, (2) the EdgeFace recognition backbone, and (3) a cycle training strategy that jointly optimizes both stages.

\subsection{Hybrid Transformer-CNN DSR Network}

\subsubsection{Architecture}

The DSR network employs a Hybrid Transformer-CNN architecture designed to capture both local details and global context while maintaining efficiency. Our implementation uses resolution-specific configurations to balance performance and parameter count (< 5.5M):

\begin{itemize}[leftmargin=*]
    \item \textbf{Shallow Feature Extraction:} A 3×3 convolutional layer maps RGB input to a base channel space (96 for 16×16, 112 for 24×24, 120 for 32×32).
    \item \textbf{Deep Feature Extraction:} A sequence of Residual Blocks (8 for 16×16, 6 for 24×24, 5 for 32×32) employing Max-Feature-Map (MFM) activation. MFM acts as a feature selection mechanism, splitting channels and taking the maximum to suppress noise and enhance discriminative features.
    \item \textbf{Global Context:} Transformer Attention Blocks (4 for 16×16/24×24, 3 for 32×32) with multi-head self-attention (8 heads) and MLP expansion ratio of 2.0. This captures long-range dependencies crucial for hallucinating plausible facial structures.
    \item \textbf{Upsampling:} Pixel shuffle layers~\cite{pixelshuffle} progressively upscale features to the target resolution.
    \item \textbf{Reconstruction:} A final 3×3 convolution produces the RGB output.
\end{itemize}

\subsubsection{Training Objectives}

DSR training employs a comprehensive loss function ($\mathcal{L}_{\text{DSR}}$) weighting identity preservation over pure pixel fidelity:

\begin{equation}
\mathcal{L}_{\text{DSR}} = \lambda_{\text{L1}}\mathcal{L}_{\text{L1}} + \lambda_{\text{P}}\mathcal{L}_{\text{P}} + \lambda_{\text{ID}}\mathcal{L}_{\text{ID}} + \lambda_{\text{Disc}}\mathcal{L}_{\text{Disc}} + \lambda_{\text{TV}}\mathcal{L}_{\text{TV}}
\end{equation}

\begin{itemize}[leftmargin=*]
    \item \textbf{Pixel Loss ($\lambda_{\text{L1}}=1.0$):} L1 distance between SR and HR images.
    \item \textbf{Perceptual Loss ($\lambda_{\text{P}}=0.02$):} Multi-scale VGG-19 feature distance.
    \item \textbf{Identity Loss ($\mathcal{L}_{\text{ID}}$):} Composed of Cosine Similarity ($\lambda=15.0-18.0$), Cross-Photo Consistency ($\lambda=3.0-4.0$), Embedding Magnitude ($\lambda=0.15$), and Feature Correlation ($\lambda=2.0-3.0$) losses, computed using the EdgeFace backbone.
    \item \textbf{Discriminative Loss ($\lambda_{\text{Disc}}=8.0$):} Ensures SR outputs of different identities remain separable in embedding space.
    \item \textbf{Total Variation ($\lambda_{\text{TV}}=10^{-5}$):} Regularization for spatial smoothness.
\end{itemize}

\subsubsection{Training Configuration}

DSR is trained for 150 epochs using the AdamW optimizer with a OneCycleLR scheduler (max LR $4 \times 10^{-4}$). We employ mixed-precision training (BF16/FP16) and gradient clipping (norm 1.0). Data augmentation includes MixUp, CutMix, horizontal flipping, rotation, and color jitter to improve robustness.

\subsection{EdgeFace Recognition Network}

\subsubsection{Architecture}

EdgeFace employs a lightweight bottleneck design optimized for mobile deployment. It uses Lightweight Depthwise Convolution (LDC) blocks to reduce FLOPs while preserving expressive power. We use the EdgeFace-XXS variant which produces 512-dimensional embeddings.

\subsubsection{ArcFace Metric Learning}

For fine-tuning EdgeFace on DSR outputs, we employ ArcFace~\cite{arcface} additive angular margin loss ($s=64, m=0.5$) to encourage intra-class compactness and inter-class separability.

\subsection{Cycle Training Strategy}

Our pipeline employs a three-phase cycle training strategy to co-adapt the super-resolution and recognition models:

\textbf{Phase 1 (DSR Training):} Train Hybrid DSR using the pretrained EdgeFace-XXS as the identity supervisor. This establishes a baseline SR capability that respects the original embedding space.

\textbf{Phase 2 (EdgeFace Fine-tuning):} Fine-tune EdgeFace on the DSR outputs from Phase 1. This adapts the recognition model to the specific distribution of super-resolved images, improving its robustness to artifacts and missing details.

\textbf{Phase 3 (DSR Retraining):} Retrain Hybrid DSR using the fine-tuned EdgeFace from Phase 2 as the supervisor. The DSR network now learns to produce features that are most discriminative for the adapted recognition model.

This cycle creates a synergistic feedback loop: DSR learns ``what features EdgeFace needs'', and EdgeFace learns ``what features DSR produces''.

\section{Experiments and Results}

\subsection{Datasets}

We evaluate our pipeline on {INSERT DATASET NAME HERE}, a frontal-face dataset with train/validation/test splits. {INSERT SUBJECT COUNT AND SPLIT DETAILS HERE}. For each subject, we select frontal-view images as high-resolution (HR) references, resized to 112×112 for recognition input.

Very-low-resolution (VLR) probes are synthesized by downsampling HR images using bicubic interpolation to 16×16, 24×24, and 32×32 pixels.

\subsection{Evaluation Metrics}

Recognition performance is measured using Rank-1 Accuracy, ROC-AUC, EER, and True Accept Rate (TAR) at fixed False Accept Rate (FAR). We report accuracy at threshold $\tau = 0.35$.

\subsection{Results and Analysis}

\subsubsection{Quantitative Performance}

Table~\ref{tab:main_results} summarizes recognition and reconstruction performance across three VLR input resolutions:

\begin{table}[h]
\centering
\caption{Recognition and Super-Resolution Performance Across VLR Resolutions}
\label{tab:main_results}
\begin{tabular}{lcccc}
\hline
\textbf{VLR Size} & \textbf{Rank-1 Acc.} & \textbf{ROC-AUC} & \textbf{EER} & \textbf{PSNR/SSIM} \\
\hline
16×16 & {INSERT} & {INSERT} & {INSERT} & {INSERT} \\
24×24 & {INSERT} & {INSERT} & {INSERT} & {INSERT} \\
32×32 & {INSERT} & {INSERT} & {INSERT} & {INSERT} \\
\hline
\end{tabular}
\end{table}

Our pipeline achieves its best performance at 32×32 VLR input. Smaller input resolutions show degraded recognition, indicating the challenge of information loss at extreme downsampling. Super-resolution quality (PSNR/SSIM) generally improves with larger inputs.

\subsubsection{Qualitative Analysis}

Visual inspection of super-resolved outputs suggests that identity-aware training produces sharper facial features compared to generic SR models optimized purely for PSNR. Our results indicate that PSNR/SSIM do not correlate perfectly with recognition performance - while 32×32 achieves high PSNR and SSIM, the corresponding Rank-1 accuracy remains modest, indicating fundamental challenges in very-low-resolution face recognition beyond reconstruction quality.

\subsubsection{Computational Efficiency}

{INSERT INFERENCE THROUGHPUT MEASUREMENTS HERE - we tested on CUDA devices but did not systematically benchmark FPS on different hardware configurations}

\subsubsection{Failure Case Analysis}

Per-subject accuracy variance is observed across the test set. Common failure modes include: (1) VLR probes where extreme downsampling destroys facial structure, (2) subjects with similar facial geometry leading to inter-subject confusion, (3) DSR introducing smoothing artifacts that obscure discriminative features. The low Rank-1 accuracy indicates that even with super-resolution, identification from such degraded inputs remains extremely challenging.

\section{Conclusion and Future Work}

This paper presented a pipeline for very-low-resolution facial recognition combining identity-aware super-resolution with lightweight recognition networks. Our key contributions include: (1) resolution-adaptive Hybrid DSR architectures trained with multi-scale perceptual and identity-preserving losses, (2) integration with EdgeFace-XXS for efficient embedding extraction, and (3) comprehensive evaluation across three VLR resolutions.

\textbf{Limitations:} Rank-1 identification accuracy remains far below practical deployment thresholds for the lowest resolutions. Our evaluation is limited to frontal-face datasets.

\textbf{Future Directions:}
\begin{enumerate}[leftmargin=*]
    \item \textbf{Attention Mechanisms:} Integrate spatial attention to focus DSR on identity-critical facial regions.
    \item \textbf{Multi-Stage Refinement:} Progressive SR with intermediate recognition losses.
    \item \textbf{Quantization and Pruning:} Apply INT8 quantization for edge deployment.
    \item \textbf{Domain Adaptation:} Fine-tune on surveillance-style datasets.
\end{enumerate}

\section*{Acknowledgments}

We thank the CMU Multi-PIE team for providing the facial recognition dataset, and the EdgeFace authors for releasing pretrained model weights.

\begin{thebibliography}{99}

\bibitem{srcnn} C. Dong, C. C. Loy, K. He, and X. Tang, ``Image super-resolution using deep convolutional networks,'' \textit{IEEE TPAMI}, vol. 38, no. 2, pp. 295-307, 2016.

\bibitem{vdsr} J. Kim, J. K. Lee, and K. M. Lee, ``Accurate image super-resolution using very deep convolutional networks,'' \textit{CVPR}, 2016.

\bibitem{edsr} B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee, ``Enhanced deep residual networks for single image super-resolution,'' \textit{CVPRW}, 2017.

\bibitem{drcn} J. Kim, J. K. Lee, and K. M. Lee, ``Deeply-recursive convolutional network for image super-resolution,'' \textit{CVPR}, 2016.

\bibitem{perceptual_loss} J. Johnson, A. Alahi, and L. Fei-Fei, ``Perceptual losses for real-time style transfer and super-resolution,'' \textit{ECCV}, 2016.

\bibitem{srgan} C. Ledig et al., ``Photo-realistic single image super-resolution using a generative adversarial network,'' \textit{CVPR}, 2017.

\bibitem{esrgan} X. Wang et al., ``ESRGAN: Enhanced super-resolution generative adversarial networks,'' \textit{ECCVW}, 2018.

\bibitem{realsr} X. Ji et al., ``Real-world super-resolution via kernel estimation and noise injection,'' \textit{CVPRW}, 2020.

\bibitem{fsrnet} Y. Chen et al., ``FSRNet: End-to-end learning face super-resolution with facial priors,'' \textit{CVPR}, 2018.

\bibitem{sparnet} C. Chen et al., ``SPARNet: Structure-preserving face hallucination,'' \textit{TIP}, 2020.

\bibitem{hennings2007} P. H. Hennings-Yeomans, S. Baker, and B. V. K. V. Kumar, ``Simultaneous super-resolution and feature extraction for recognition of low-resolution faces,'' \textit{CVPR}, 2007.

\bibitem{biswas2011} S. Biswas, K. W. Bowyer, and P. J. Flynn, ``Multidimensional scaling for matching low-resolution face images,'' \textit{IEEE TPAMI}, vol. 34, no. 10, pp. 2019-2030, 2012.

\bibitem{coupledgan} B. Li et al., ``Towards domain-invariant face recognition via coupled generative adversarial networks,'' \textit{AAAI}, 2018.

\bibitem{lcnn} J. Chen, Y. Deng, G. Bai, and G. Su, ``Face super-resolution through wasserstein GANs,'' \textit{arXiv preprint}, 2017.

\bibitem{dsr} C. He et al., ``Deep super-resolution for face recognition,'' \textit{Pattern Recognition}, vol. 107, 2020.

\bibitem{dran} X. Xu, J. Sun, and X. Cheng, ``Domain adaptation for low-resolution face recognition,'' \textit{ICIP}, 2019.

\bibitem{dade} K. Cao et al., ``Learning resolution-invariant deep representations for person re-identification,'' \textit{AAAI}, 2019.

\bibitem{mobilefacenet} S. Chen et al., ``MobileFaceNets: Efficient CNNs for accurate real-time face verification on mobile devices,'' \textit{CCBR}, 2018.

\bibitem{shufflefacenet} M. Martindez-Diaz et al., ``ShuffleFaceNet: A lightweight face architecture for efficient and highly-accurate face recognition,'' \textit{ICCVW}, 2019.

\bibitem{edgeface} G. F. Boutros et al., ``EdgeFace: Efficient face recognition model for edge devices,'' \textit{arXiv preprint arXiv:2307.01838}, 2023.

\bibitem{identity_aware_sr} X. Yu and F. Porikli, ``Hallucinating very low-resolution unaligned and noisy face images by transformative discriminative autoencoders,'' \textit{CVPR}, 2017.

\bibitem{learn_sr} Y. Xu et al., ``Learning to super-resolve for low-resolution face recognition,'' \textit{Pattern Recognition}, vol. 107, 2020.

\bibitem{pixelshuffle} W. Shi et al., ``Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network,'' \textit{CVPR}, 2016.

\bibitem{arcface} J. Deng, J. Guo, N. Xue, and S. Zafeiriou, ``ArcFace: Additive angular margin loss for deep face recognition,'' \textit{CVPR}, 2019.

{INSERT DATASET CITATION IF NEEDED}

\end{thebibliography}

\end{document}