{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LRFR Project - Google Colab Training Notebook\n",
    "\n",
    "This notebook trains the Low-Resolution Facial Recognition (LRFR) models on Google Colab using an A100 GPU.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Code:** Cloned from GitHub (latest version).\n",
    "2.  **Data:** Loaded from `dataset.zip` on Google Drive (fast I/O).\n",
    "\n",
    "## Prerequisites\n",
    "1.  **Zip Dataset:** Zip your `technical/dataset` folder into `dataset.zip` and upload it to your Google Drive (e.g., in a `LRFR_Project` folder).\n",
    "2.  **Runtime:** Change runtime to **GPU (A100)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for dataset and saving results)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "GITHUB_REPO = 'https://github.com/dszurek/LRFR-Project.git'\n",
    "PROJECT_ROOT = '/content/LRFR-Project'\n",
    "# Path to dataset.zip on Drive. Adjust this if you put it in a subfolder.\n",
    "DRIVE_ZIP_PATH = '/content/drive/MyDrive/LRFR_Project/dataset.zip'\n",
    "# ---------------------\n",
    "\n",
    "# Clone Repository\n",
    "if os.path.exists(PROJECT_ROOT):\n",
    "    print(\"Repository already exists. Pulling latest changes...\")\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    !git pull\n",
    "else:\n",
    "    print(\"Cloning repository...\")\n",
    "    os.chdir('/content')\n",
    "    !git clone {GITHUB_REPO}\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "We unzip `dataset.zip` from Drive to the local VM for high-speed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for dataset zip on Drive\n",
    "if not os.path.exists(DRIVE_ZIP_PATH):\n",
    "    # Try looking in root if not found in folder\n",
    "    DRIVE_ZIP_PATH_ALT = '/content/drive/MyDrive/dataset.zip'\n",
    "    if os.path.exists(DRIVE_ZIP_PATH_ALT):\n",
    "        DRIVE_ZIP_PATH = DRIVE_ZIP_PATH_ALT\n",
    "        print(f\"Found dataset.zip at root: {DRIVE_ZIP_PATH}\")\n",
    "    else:\n",
    "        print(f\"❌ ERROR: dataset.zip not found at {DRIVE_ZIP_PATH} or {DRIVE_ZIP_PATH_ALT}\")\n",
    "        print(\"Please upload dataset.zip to your Google Drive.\")\n",
    "\n",
    "if os.path.exists(DRIVE_ZIP_PATH):\n",
    "    print(f\"Unzipping {DRIVE_ZIP_PATH} to local VM...\")\n",
    "    \n",
    "    # Unzip to /content/temp_dataset first to inspect structure\n",
    "    temp_extract = '/content/temp_dataset'\n",
    "    if os.path.exists(temp_extract):\n",
    "        shutil.rmtree(temp_extract)\n",
    "    shutil.unpack_archive(DRIVE_ZIP_PATH, temp_extract)\n",
    "    \n",
    "    # Move to expected location: PROJECT_ROOT/technical/dataset\n",
    "    target_dir = os.path.join(PROJECT_ROOT, 'technical', 'dataset')\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if it unzipped as 'dataset' folder or contents directly\n",
    "    if os.path.exists(os.path.join(temp_extract, 'dataset')):\n",
    "        source = os.path.join(temp_extract, 'dataset')\n",
    "    elif os.path.exists(os.path.join(temp_extract, 'train_processed')):\n",
    "        source = temp_extract\n",
    "    elif os.path.exists(os.path.join(temp_extract, 'technical', 'dataset')):\n",
    "        source = os.path.join(temp_extract, 'technical', 'dataset')\n",
    "    else:\n",
    "        source = temp_extract # Hope for the best or it's a flat structure\n",
    "        \n",
    "    # Move contents\n",
    "    for item in os.listdir(source):\n",
    "        s = os.path.join(source, item)\n",
    "        d = os.path.join(target_dir, item)\n",
    "        if os.path.exists(d):\n",
    "            if os.path.isdir(d):\n",
    "                shutil.rmtree(d)\n",
    "            else:\n",
    "                os.remove(d)\n",
    "        shutil.move(s, d)\n",
    "        \n",
    "    print(f\"✅ Dataset ready at {target_dir}\")\n",
    "    shutil.rmtree(temp_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa7fde",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "### 3.1 Train Hybrid DSR Models\n",
    "Train the Hybrid Transformer-CNN Super-Resolution models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6745267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Hybrid DSR (16, 24, 32)\n",
    "!python -m technical.dsr.train_hybrid_dsr --vlr-size 16\n",
    "!python -m technical.dsr.train_hybrid_dsr --vlr-size 24\n",
    "!python -m technical.dsr.train_hybrid_dsr --vlr-size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f563eb",
   "metadata": {},
   "source": [
    "### 3.2 Fine-tune EdgeFace\n",
    "Fine-tune the face recognition model for each resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd598bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune EdgeFace\n",
    "!python -m technical.facial_rec.finetune_edgeface --vlr-size 16 --device cuda\n",
    "!python -m technical.facial_rec.finetune_edgeface --vlr-size 24 --device cuda\n",
    "!python -m technical.facial_rec.finetune_edgeface --vlr-size 32 --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd19762e",
   "metadata": {},
   "source": [
    "## 4. Evaluation & Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d19f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m technical.pipeline.evaluate_cli --resolutions 16 24 32 --output-dir evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a43f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results back to Drive\n",
    "import glob\n",
    "SAVE_DIR = '/content/drive/MyDrive/LRFR_Project/results'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Saving models and results to {SAVE_DIR}...\")\n",
    "\n",
    "# Copy DSR models (both standard and hybrid naming patterns just in case)\n",
    "for f in glob.glob('technical/dsr/*.pth'):\n",
    "    shutil.copy(f, SAVE_DIR)\n",
    "\n",
    "# Copy EdgeFace weights\n",
    "for f in glob.glob('technical/facial_rec/edgeface_weights/*.pth'):\n",
    "    shutil.copy(f, SAVE_DIR)\n",
    "\n",
    "# Copy Evaluation Results\n",
    "if os.path.exists('evaluation_results'):\n",
    "    eval_dest = os.path.join(SAVE_DIR, 'evaluation_results')\n",
    "    if os.path.exists(eval_dest):\n",
    "        shutil.rmtree(eval_dest)\n",
    "    shutil.copytree('evaluation_results', eval_dest)\n",
    "\n",
    "print(\"✅ All results saved to Drive!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
